{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "updatedHW3Flavio.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UfhM776xVZfA",
        "lgT7uzP150u3",
        "kxOv_B_N6Jxc",
        "Z4E_1MdWV0-e",
        "PqvRWJaSWA3z",
        "t1YJ8xjfzNCo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFKSKW42SCyu"
      },
      "source": [
        "!python -m spacy download en_core_web_md #Restart runtime after executing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSWaWR2g2DxL"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from progressbar import Bar, ETA, Percentage, ProgressBar\n",
        "from itertools import zip_longest\n",
        "from keras.models import load_model\n",
        "from os import listdir\n",
        "from keras.utils import np_utils, generic_utils\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input \n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "from keras import Input\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "\n",
        "# Set the seed for random operations. \n",
        "# This let our experiments to be reproducible. \n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Get current working directory\n",
        "cwd = os.getcwd()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjnh11hz2Fpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ed48b2-bc58-4b1e-b7ce-82a19af226e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5Ubba9b9QN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2186d8-04d6-4e4d-8a16-7413d15eccf3"
      },
      "source": [
        "!unzip /content/drive/My\\ Drive/VQA_Dataset.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/VQA_Dataset.zip\n",
            "replace VQA_Dataset/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwQmteo19_cI"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "yl0jH9THx3F9",
        "outputId": "adb17721-be27-4a33-b848-56d149df92a3"
      },
      "source": [
        "cwd"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpJbCIqh48Vw"
      },
      "source": [
        "## Creation of Feature Maps from images\n",
        "In this section we create feature maps from the input images using VGG. We decided ti split this part from the rest of the model so it could be implemented in a separate notebook to reduce workload.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESGPC2Vc41G9",
        "outputId": "4d753fa5-3ebf-4319-d735-872b058500fe"
      },
      "source": [
        "IMG_W =  175\n",
        "IMG_H = 100\n",
        "BS = 16\n",
        "\n",
        "dataset_dir = os.path.join(cwd, \"VQA_Dataset\")\n",
        "test_dir = os.path.join(dataset_dir, 'Images')\n",
        "\n",
        "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_gen = test_data_gen.flow_from_directory(dataset_dir,\n",
        "                                             batch_size=BS,\n",
        "                                             color_mode=\"rgb\",\n",
        "                                             target_size=(IMG_H, IMG_W),\n",
        "                                             shuffle=False)\n",
        "test_gen.reset()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 29333 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBgux3Or4_pG"
      },
      "source": [
        "#VGG model\n",
        "vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMG_H, IMG_W, 3)) \n",
        "vgg.trainable = False\n",
        "vgg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3RK4g2f5BtG",
        "outputId": "d084b68d-390a-4187-b450-419a9ec2e511"
      },
      "source": [
        "pre_model = tf.keras.Sequential()\n",
        "pre_model.add(vgg)\n",
        "pre_model.trainable = False\n",
        "img_predictions = pre_model.predict(test_gen, len(test_gen), verbose=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1834/1834 [==============================] - 284s 151ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvoAMjGR5FkV"
      },
      "source": [
        "feature_maps = {}\n",
        "image_names = []\n",
        "\n",
        "from os import listdir\n",
        "\n",
        "for name in listdir(test_dir):\n",
        "  image_names.append(name)\n",
        "\n",
        "image_names.sort()\n",
        "\n",
        "for i in range(len(image_names)):\n",
        "  img_name=image_names[i].replace('.png' , '')\n",
        "  feature_maps[img_name] = img_predictions[i]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfhM776xVZfA"
      },
      "source": [
        "## Creation on input Questions and answer vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxyj8LagWV_2"
      },
      "source": [
        "# OPEN QUESTIONS\n",
        "file=os.path.join(dataset_dir, 'train_questions_annotations.json')\n",
        "f = open(file)\n",
        "train_q = json.load(f)\n",
        "\n",
        "#Load Spacy\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUM4_d2A2Hok"
      },
      "source": [
        "#Question list\n",
        "questions = list([text for id in train_q.keys() for text in train_q[id]['question'].splitlines()])\n",
        "\n",
        "#Answer list\n",
        "answers = list([text for id in train_q.keys() for text in train_q[id]['answer'].splitlines()])\n",
        "\n",
        "answer_vocab = list(set([text for id in train_q.keys() for text in train_q[id]['answer'].splitlines()]))\n",
        "answer_vocab_size = len(answer_vocab)\n",
        "\n",
        "ans_word_to_idx = { w: i for i, w in enumerate(answer_vocab) }\n",
        "ans_idx_to_word = { i: w for i, w in enumerate(answer_vocab) }\n",
        "\n",
        "#Img_id list\n",
        "img_ids = list([text for id in train_q.keys() for text in train_q[id]['image_id'].splitlines()])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL-_HZXi8vim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c15067f8-b7bb-4b0f-f8fa-80d13e82f71d"
      },
      "source": [
        "#Check correctness\n",
        "print(list(train_q.items())[0])\n",
        "print(answers[0])\n",
        "print(img_ids[0])\n",
        "print(questions[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('117792', {'question': 'Who looks happier?', 'image_id': '11779', 'answer': 'man'})\n",
            "man\n",
            "11779\n",
            "Who looks happier?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dRZT9Tx-eFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59711f5-a482-4136-9a61-dcfbc800835d"
      },
      "source": [
        "### Find max len quesiton\n",
        "max_len = 0\n",
        "max_len_q_index = 0\n",
        "for i in range(len(questions)):\n",
        "  q_len = len(questions[i].strip().split(\" \"))\n",
        "  if(q_len > max_len):\n",
        "    max_len = q_len\n",
        "    max_len_q_index = i\n",
        "\n",
        "max_len = len(nlp(questions[max_len_q_index]))\n",
        "\n",
        "print(max_len)\n",
        "print(list(train_q.items())[max_len_q_index])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24\n",
            "('81082', {'question': 'Is it likely that it is winter?  The human would say no since there are no coats on the coat rack?', 'image_id': '8108', 'answer': 'no'})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgT7uzP150u3"
      },
      "source": [
        "## Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t9OZg4g526r"
      },
      "source": [
        "#create inputs\n",
        "def createInputs(text):\n",
        "  '''\n",
        "  Returns an array of one-hot vectors representing the words\n",
        "  in the input text string.\n",
        "  - text is a string\n",
        "  - Each one-hot vector has shape (vocab_size, 1)\n",
        "  '''\n",
        "  inputs = []\n",
        "  for w in text.split(' '):\n",
        "    v = np.zeros((vocab_size, 1)) \n",
        "    v[word_to_idx[w]] = 1\n",
        "    inputs.append(v)\n",
        "  return inputs\n",
        "\n",
        "def get_questions_tensor_timeseries(questions, nlp, timesteps):\n",
        "    #assert not isinstance(questions, list) --- CHECK WHEN CREATING BATCHES\n",
        "    nb_samples = len(questions)\n",
        "    word_vec_dim = nlp(questions[0])[0].vector.shape[0]\n",
        "    questions_tensor = np.zeros((nb_samples, timesteps, word_vec_dim))\n",
        "    for i in range(len(questions)):\n",
        "        tokens = nlp(questions[i])\n",
        "        for j in range(len(tokens)):\n",
        "            if j<timesteps:\n",
        "                questions_tensor[i,j,:] = tokens[j].vector\n",
        "    return questions_tensor\n",
        "\n",
        "#Encode answers\n",
        "def encodeAns(w, ans_word_to_idx):\n",
        "  '''\n",
        "  Returns a one-hot vectors representing the answer word in the answer vocab.\n",
        "  Each one-hot vector has shape (answer_vocab_size, 1)\n",
        "  '''\n",
        "  ohe_word = np.zeros((answer_vocab_size, 1)) \n",
        "  ohe_word[ans_word_to_idx[w]] = 1\n",
        "  return ohe_word \n",
        "\n",
        "#Decode answers\n",
        "def decodeAns(ohe_w, ans_idx_to_word):\n",
        "  '''\n",
        "  Returns the word from the one-hot-encoding answer vector\n",
        "  '''\n",
        "  i = 0\n",
        "  while(ohe_w[i] == 0):\n",
        "    i = i+1\n",
        "  return ans_idx_to_word[i] \n",
        "\n",
        "def get_questions_tensor_timeseries(questions, nlp, timesteps):\n",
        "    #assert not isinstance(questions, list) #--- CHECK WHEN CREATING BATCHES\n",
        "    nb_samples = len(questions)\n",
        "    word_vec_dim = nlp(questions[0])[0].vector.shape[0]\n",
        "    questions_tensor = np.zeros((nb_samples, timesteps, word_vec_dim))\n",
        "    for i in range(len(questions)):\n",
        "        tokens = nlp(questions[i])\n",
        "        for j in range(len(tokens)):\n",
        "            if j<timesteps:\n",
        "                questions_tensor[i,j,:] = tokens[j].vector\n",
        "    return questions_tensor\n",
        "\n",
        "def get_question_tensor_timeseries(question, nlp, timesteps): \n",
        "    word_vec_dim = nlp(question)[0].vector.shape[0]\n",
        "    question_tensor = np.zeros((timesteps, word_vec_dim))\n",
        "    tokens = nlp(question)\n",
        "    for j in range(len(tokens)):\n",
        "      if j<timesteps:\n",
        "        question_tensor[j,:] = tokens[j].vector\n",
        "    return question_tensor \n",
        "\n",
        "def grouped(iterable, n, fillvalue=None):\n",
        "    args = [iter(iterable)] * n\n",
        "   #print(args)\n",
        "    #for a in(zip_longest(*args, fillvalue=fillvalue)):\n",
        "        #print(a)\n",
        "    return zip_longest(*args, fillvalue=fillvalue)      "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAGfc1mw6B8N"
      },
      "source": [
        "## Input Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSBhggSr8it8"
      },
      "source": [
        "def create_tuples(questions, answers, img_ids, max_len, nlp, ans_word_to_idx, feature_maps):\n",
        "    \n",
        "    new_answers_train = list()\n",
        "    new_questions_train = list()\n",
        "    new_images_train = list()\n",
        "    for ans, ques, img_id in zip(answers, questions, img_ids):\n",
        "      \n",
        "      ohe_ans = encodeAns(ans, ans_word_to_idx) #one hot encoding of answer\n",
        "      encoded_question = get_question_tensor_timeseries(ques, nlp, max_len)  #question embedding\n",
        "      img_feature_maps = feature_maps[img_id] #feature maps of that img\n",
        "\n",
        "      new_answers_train.append(ohe_ans)\n",
        "      new_questions_train.append(encoded_question)\n",
        "      new_images_train.append(img_feature_maps.flatten())\n",
        "\n",
        "    return (new_questions_train, new_answers_train, new_images_train)\n",
        "\n",
        "def create_test_tuples(questions, img_ids, max_len, nlp, feature_maps):\n",
        "    \n",
        "    new_questions_train = list()\n",
        "    new_images_train = list()\n",
        "\n",
        "    for ques, img_id in zip(questions, img_ids):\n",
        "      \n",
        "      encoded_question = get_question_tensor_timeseries(ques, nlp, max_len)  #question embedding\n",
        "      img_feature_maps = feature_maps[img_id] #feature maps of that img\n",
        "\n",
        "      new_questions_train.append(encoded_question)\n",
        "      new_images_train.append(img_feature_maps.flatten())\n",
        "\n",
        "    return (new_questions_train, new_images_train)    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4PUkCtR2KCg"
      },
      "source": [
        "#create lists of encoded items\n",
        "en_questions, en_answers, en_img_ids = create_tuples(questions, answers, img_ids, max_len, nlp, ans_word_to_idx, feature_maps)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax0wUm4ZJNQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f91da4-d892-4dab-fd9a-540b783bdf94"
      },
      "source": [
        "print (len(en_questions), len(en_answers),len(en_img_ids))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58832 58832 58832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxOv_B_N6Jxc"
      },
      "source": [
        "## Image Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-B5ffI6nyJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a64135-6c92-4384-b6fc-0b89115a0db0"
      },
      "source": [
        "#With load feature map from file should just be an empty model\n",
        "image_model = Sequential()\n",
        "image_model.add(Reshape(input_shape = (7680,), target_shape=(7680,))) ### set correct shape 7680 (175x100)\n",
        "model1 = Model(inputs = image_model.input, outputs = image_model.output)\n",
        "model1.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_input (InputLayer)   [(None, 7680)]            0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 7680)              0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4E_1MdWV0-e"
      },
      "source": [
        "## LSTM Model\n",
        "LSTM model that takes the question as input and creates the output to send to our final dense model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCzOv3cQpkBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc87540a-9f50-4b75-ed9d-42194cd781e7"
      },
      "source": [
        "# Question Model\n",
        "# LSTM RNN\n",
        "word2vec_dim = 300 \n",
        "num_layers_lstm = 3\n",
        "num_hidden_nodes_lstm = 512\n",
        "output_dim = num_hidden_nodes_lstm\n",
        "\n",
        "language_model = Sequential()\n",
        "language_model.add(LSTM(units=output_dim, \n",
        "                        return_sequences=True, input_shape=(None, word2vec_dim)))\n",
        "\n",
        "for i in range(num_layers_lstm-2):\n",
        "    language_model.add(LSTM(units=output_dim, return_sequences=True))\n",
        "language_model.add(LSTM(units=output_dim, return_sequences=False))\n",
        "\n",
        "model2 = Model(language_model.input, language_model.output)\n",
        "model2.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_input (InputLayer)      [(None, None, 300)]       0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 512)         1665024   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 512)         2099200   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 512)               2099200   \n",
            "=================================================================\n",
            "Total params: 5,863,424\n",
            "Trainable params: 5,863,424\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqvRWJaSWA3z"
      },
      "source": [
        "## Final model \n",
        "Model that merges the questions and the images and has dense layers after it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5aoze4zp4a2"
      },
      "source": [
        "# Merge\n",
        "combined = concatenate([image_model.output, language_model.output])\n",
        "model = Dense(512, activation = 'relu')(combined)\n",
        "model = Dropout(0.3)(model)\n",
        "\n",
        "model = Dense(128, activation = 'relu')(model)\n",
        "model = Dropout(0.3)(model)\n",
        "\n",
        "model = Dense(58)(model)\n",
        "model = Activation(\"softmax\")(model)\n",
        "\n",
        "model = Model(inputs=[image_model.input, language_model.input], outputs=model)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R87oDkx4Ng4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419f6db7-7ff0-42ba-f554-280f0a33ba18"
      },
      "source": [
        "lr = 1e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Validation metrics\n",
        "\n",
        "metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics = metric)\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "lstm_input (InputLayer)         [(None, None, 300)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, None, 512)    1665024     lstm_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_input (InputLayer)      [(None, 7680)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, None, 512)    2099200     lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 7680)         0           reshape_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 512)          2099200     lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 8192)         0           reshape[0][0]                    \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          4194816     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          65664       dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 58)           7482        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 58)           0           dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 10,131,386\n",
            "Trainable params: 10,131,386\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1YJ8xjfzNCo"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFj4ZVqRO_5G"
      },
      "source": [
        "train_questions = en_questions[:40000]\n",
        "train_answers = en_answers[:40000]\n",
        "train_image_id = en_img_ids[:40000]\n",
        "\n",
        "valid_questions = en_questions[40000:]\n",
        "valid_answers = en_answers[40000:]\n",
        "valid_image_id = en_img_ids[40000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6do_uJRPIAB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b053e959-1697-44f6-dbdf-882da2a24856"
      },
      "source": [
        "batch_size = 33\n",
        "vbatch_size= 10\n",
        "num_epochs = 10\n",
        "\n",
        "for k in range(num_epochs):\n",
        "    print(\"Epoch Number: \",k+1)\n",
        "    progbar = generic_utils.Progbar(len(train_questions))\n",
        "    for question_batch, ans_batch, im_batch, vquestion_batch, vans_batch, vim_batch, in zip(grouped(train_questions, batch_size, fillvalue=train_questions[-1]), \n",
        "                                                                                            grouped(train_answers, batch_size, fillvalue=train_answers[-1]),\n",
        "                                                                                            grouped(train_image_id, batch_size, fillvalue=train_image_id[-1]),\n",
        "                                                                                            grouped(valid_questions, vbatch_size, fillvalue=valid_questions[-1]), \n",
        "                                                                                            grouped(valid_answers, vbatch_size, fillvalue=valid_answers[-1]),\n",
        "                                                                                            grouped(valid_image_id, vbatch_size, fillvalue=valid_image_id[-1])):\n",
        "                                                X_ques_batch = question_batch\n",
        "                                                X_img_batch = im_batch\n",
        "                                                Y_batch = ans_batch\n",
        "                                                X_vques_batch = vquestion_batch\n",
        "                                                X_vimg_batch = vim_batch\n",
        "                                                Y_vbatch = vans_batch\n",
        "                                                np_X_ques_batch = np.array(X_ques_batch)\n",
        "                                                np_X_img_batch = np.array(X_img_batch)\n",
        "                                                np_Y_batch = np.array(Y_batch)\n",
        "                                                np_X_vques_batch = np.array(X_vques_batch)\n",
        "                                                np_X_vimg_batch = np.array(X_vimg_batch)\n",
        "                                                np_Y_vbatch = np.array(Y_vbatch)\n",
        "                                                loss, acc = model.train_on_batch(({'lstm_input' : np_X_ques_batch, 'reshape_input' : np_X_img_batch}), np_Y_batch, class_weight=None)\n",
        "                                                vloss, vacc = model.test_on_batch(({'lstm_input' : np_X_vques_batch, 'reshape_input' : np_X_vimg_batch}), np_Y_vbatch)\n",
        "                                                #loss = model.train_on_batch(({'lstm_1_input' : np_X_ques_batch, 'reshape_1_input' : np_X_img_batch}), np_Y_batch, class_weight=None)\n",
        "                                                progbar.add(batch_size, values=[('train loss', loss),('train accuracy', acc),('val loss', vloss),('val accuracy', vacc)])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Number:  1\n",
            "45012/45000 [==============================] - 54s 1ms/step - train loss: 2.0247 - train accuracy: 0.3662 - val loss: 1.8806 - val accuracy: 0.4087\n",
            "Epoch Number:  2\n",
            "45012/45000 [==============================] - 51s 1ms/step - train loss: 1.6995 - train accuracy: 0.3986 - val loss: 1.5986 - val accuracy: 0.4304\n",
            "Epoch Number:  3\n",
            "45012/45000 [==============================] - 51s 1ms/step - train loss: 1.6495 - train accuracy: 0.4133 - val loss: 1.5729 - val accuracy: 0.4328\n",
            "Epoch Number:  4\n",
            "45012/45000 [==============================] - 51s 1ms/step - train loss: 1.5862 - train accuracy: 0.4243 - val loss: 1.5341 - val accuracy: 0.4369\n",
            "Epoch Number:  5\n",
            "45012/45000 [==============================] - 51s 1ms/step - train loss: 1.5535 - train accuracy: 0.4304 - val loss: 1.5198 - val accuracy: 0.4408\n",
            "Epoch Number:  6\n",
            "45012/45000 [==============================] - 51s 1ms/step - train loss: 1.4940 - train accuracy: 0.4335 - val loss: 1.4773 - val accuracy: 0.4434\n",
            "Epoch Number:  7\n",
            "45012/45000 [==============================] - 51s 1ms/step - train loss: 1.4523 - train accuracy: 0.4414 - val loss: 1.4426 - val accuracy: 0.4465\n",
            "Epoch Number:  8\n",
            "45012/45000 [==============================] - 51s 1ms/step - train loss: 1.4075 - train accuracy: 0.4469 - val loss: 1.4157 - val accuracy: 0.4511\n",
            "Epoch Number:  9\n",
            "45012/45000 [==============================] - 52s 1ms/step - train loss: 1.3819 - train accuracy: 0.4528 - val loss: 1.4039 - val accuracy: 0.4528\n",
            "Epoch Number:  10\n",
            "45012/45000 [==============================] - 51s 1ms/step - train loss: 1.3279 - train accuracy: 0.4607 - val loss: 1.3630 - val accuracy: 0.4588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tEeT2tP5jtX"
      },
      "source": [
        "widgets = ['Evaluating ', Percentage(), ' ', Bar(marker='#',left='[',right=']'), ' ', ETA()]\n",
        "pbar = ProgressBar(widgets=widgets)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3g2iwTd5BBK"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#Check scores\n",
        "valid_pred = []\n",
        "batch_size = 1 \n",
        "\n",
        "for qu_batch,an_batch,im_batch in pbar(zip(grouped(valid_questions, batch_size, \n",
        "                                                   fillvalue=valid_questions[0]), \n",
        "                                           grouped(valid_answers, batch_size, \n",
        "                                                   fillvalue=valid_answers[0]), \n",
        "                                           grouped(valid_image_id, batch_size, \n",
        "                                                   fillvalue=valid_image_id[0]))):\n",
        "    X_ques_batch = qu_batch\n",
        "    X_img_batch = im_batch\n",
        "    np_X_ques_batch = np.array(X_ques_batch)\n",
        "    np_X_img_batch = np.array(X_img_batch)\n",
        "    valid_predict = model.predict(({'lstm_input' : np_X_ques_batch, 'reshape_input' : np_X_img_batch}))\n",
        "    valid_predict = np.argmax(valid_predict,axis=1)\n",
        "    valid_pred.extend(valid_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc-Yz29I8OZd"
      },
      "source": [
        "#Decode answers to words\n",
        "for i in range(len(valid_pred)):\n",
        "  vector = np.zeros(len(answer_vocab))\n",
        "  vector[valid_pred[i]] = 1\n",
        "  valid_pred[i] = decodeAns(vector, ans_idx_to_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aCM23uxCqYv"
      },
      "source": [
        "#Check same length\n",
        "print(len(valid_pred))\n",
        "print(len(valid_answers))\n",
        "#Check correctness\n",
        "print(valid_pred[2])\n",
        "print(decodeAns(valid_answers[2], ans_idx_to_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI8p-2IICEb_"
      },
      "source": [
        "#Manual accuracy over validation set\n",
        "correct = 0\n",
        "for j in range(len(valid_pred)):\n",
        "  if(valid_pred[j] == decodeAns(valid_answers[j], ans_idx_to_word)):\n",
        "    correct = correct + 1\n",
        "\n",
        "print(correct)\n",
        "\n",
        "accuracy = correct/len(valid_pred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc9ZwkE4zSGE"
      },
      "source": [
        "## Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCwI8VHnzESB"
      },
      "source": [
        "path = os.path.join(dataset_dir, 'test_questions.json')\n",
        "d = open(path)\n",
        "test_q = json.load(d)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmxdx0LozlUZ"
      },
      "source": [
        "#Test questions\n",
        "test_questions = list([text for id in test_q.keys() for text in test_q[id]['question'].splitlines()])\n",
        "\n",
        "#Test Img_id list \n",
        "test_img_ids = list([text for id in test_q.keys() for text in test_q[id]['image_id'].splitlines()])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwLwUaMa0kyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf617e8-53d6-428c-febc-1b20a446cdf2"
      },
      "source": [
        "test_max_len = 0\n",
        "test_max_len_q_index = 0\n",
        "for i in range(len(test_questions)):\n",
        "  test_q_len = len(test_questions[i].strip().split(\" \"))\n",
        "  if(test_q_len > test_max_len):\n",
        "    test_max_len = test_q_len\n",
        "    test_max_len_q_index = i\n",
        "\n",
        "test_max_len = len(nlp(test_questions[test_max_len_q_index]))\n",
        "\n",
        "print(test_max_len)\n",
        "print(max_len)\n",
        "print(list(test_q.items())[test_max_len_q_index])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19\n",
            "24\n",
            "('21471', {'question': 'Is the lady standing on the rug and the woman in the portrait wearing the same colored shirt?', 'image_id': '2147'})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKVKMhYOHI7W"
      },
      "source": [
        "#ONLY IF NEEDED To avoid running out of RAM after training delete encoded training questions \n",
        "del train_questions\n",
        "del en_questions\n",
        "del valid_questions\n",
        "\n",
        "import gc\n",
        "gc.collect"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM5vMK641a2h"
      },
      "source": [
        "en_test_questions, en_test_img_ids = create_test_tuples(test_questions, test_img_ids, max_len, nlp, feature_maps)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B620O6CzfTf",
        "outputId": "5a4025cc-2863-41a4-b4d2-11c06c76c4f2"
      },
      "source": [
        "print(len(test_questions))\n",
        "print(len(en_test_questions))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6372\n",
            "6372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxmwdDZ72TOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a212ba8-b6a8-4562-947b-5894b100b45e"
      },
      "source": [
        "test_batch_size = 1\n",
        "y_pred = []\n",
        "\n",
        "\n",
        "for qu_batch,im_batch in pbar(zip(grouped(en_test_questions, test_batch_size, \n",
        "                                                   fillvalue=en_test_questions[0]),                                            \n",
        "                                           grouped(en_test_img_ids, test_batch_size, \n",
        "                                                   fillvalue=en_test_img_ids[0]))):\n",
        "\n",
        "    X_ques_batch = qu_batch\n",
        "    X_img_batch = im_batch\n",
        "    np_X_ques_batch = np.array(X_ques_batch)\n",
        "    np_X_img_batch = np.array(X_img_batch)\n",
        "    y_predict = model.predict(({'lstm_input' : np_X_ques_batch, 'reshape_input' : np_X_img_batch}))\n",
        "    y_predict = np.argmax(y_predict,axis=1)\n",
        "    y_pred.extend(y_predict)\n",
        "    "
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating N/A% [#                                             ] Time:  1:10:19"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTDqZBEjqMJe"
      },
      "source": [
        "## Creation of submission csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YchCUyNTZguc"
      },
      "source": [
        "labels_dict = {\n",
        "        '0': 0,\n",
        "        '1': 1,\n",
        "        '2': 2,\n",
        "        '3': 3,\n",
        "        '4': 4,\n",
        "        '5': 5,\n",
        "        'apple': 6,\n",
        "        'baseball': 7,\n",
        "        'bench': 8,\n",
        "        'bike': 9,\n",
        "        'bird': 10,\n",
        "        'black': 11,\n",
        "        'blanket': 12,\n",
        "        'blue': 13,\n",
        "        'bone': 14,\n",
        "        'book': 15,\n",
        "        'boy': 16,\n",
        "        'brown': 17,\n",
        "        'cat': 18,\n",
        "        'chair': 19,\n",
        "        'couch': 20,\n",
        "        'dog': 21,\n",
        "        'floor': 22,\n",
        "        'food': 23,\n",
        "        'football': 24,\n",
        "        'girl': 25,\n",
        "        'grass': 26,\n",
        "        'gray': 27,\n",
        "        'green': 28,\n",
        "        'left': 29,\n",
        "        'log': 30,\n",
        "        'man': 31,\n",
        "        'monkey bars': 32,\n",
        "        'no': 33,\n",
        "        'nothing': 34,\n",
        "        'orange': 35,\n",
        "        'pie': 36,\n",
        "        'plant': 37,\n",
        "        'playing': 38,\n",
        "        'red': 39,\n",
        "        'right': 40,\n",
        "        'rug': 41,\n",
        "        'sandbox': 42,\n",
        "        'sitting': 43,\n",
        "        'sleeping': 44,\n",
        "        'soccer': 45,\n",
        "        'squirrel': 46,\n",
        "        'standing': 47,\n",
        "        'stool': 48,\n",
        "        'sunny': 49,\n",
        "        'table': 50,\n",
        "        'tree': 51,\n",
        "        'watermelon': 52,\n",
        "        'white': 53,\n",
        "        'wine': 54,\n",
        "        'woman': 55,\n",
        "        'yellow': 56,\n",
        "        'yes': 57\n",
        "}\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUhT-TpBotfM"
      },
      "source": [
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlgZSaTkOXvm"
      },
      "source": [
        "#Decode answers to words\n",
        "de_y_pred = y_pred\n",
        "for i in range(len(de_y_pred)):\n",
        "  vector = np.zeros(len(answer_vocab))\n",
        "  vector[de_y_pred[i]] = 1\n",
        "  de_y_pred[i] = decodeAns(vector, ans_idx_to_word)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZfYZCqPP1YX"
      },
      "source": [
        "#Re encode with correct dictionary\n",
        "final_y_pred = de_y_pred\n",
        "for i in range(len(final_y_pred)):\n",
        "  final_y_pred[i] = labels_dict.get(final_y_pred[i])"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxWaAKtCWHgU"
      },
      "source": [
        "#create submission\n",
        "results = {}\n",
        "test_ids = list(test_q.keys())\n",
        "for i in range(len(test_ids)):\n",
        "  results[test_ids[i]] = final_y_pred[i]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41_v103bWUpl"
      },
      "source": [
        "create_csv(results, '/content/drive/My Drive')"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzmoK3ZMQBKB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}