{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RFKSKW42SCyu"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md #Restart runtime after executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSWaWR2g2DxL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from progressbar import Bar, ETA, Percentage, ProgressBar\n",
    "from itertools import zip_longest\n",
    "from keras.models import load_model\n",
    "from os import listdir\n",
    "from keras.utils import np_utils, generic_utils\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from keras import Input\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zjnh11hz2Fpi",
    "outputId": "45ff9bea-5f7f-4f6b-b0b0-56dd5ca56f29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5Ubba9b9QN4"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/My\\ Drive/VQA_Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwQmteo19_cI"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/MyDrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpJbCIqh48Vw"
   },
   "source": [
    "## Creation of Feature Maps from images\n",
    "In this section we create feature maps from the input images using VGG. We decided ti split this part from the rest of the model so it could be implemented in a separate notebook to reduce workload.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ESGPC2Vc41G9",
    "outputId": "c4aef9e6-10bb-4100-a4c5-4c095d85ab57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29333 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "IMG_W =  175\n",
    "IMG_H = 100\n",
    "BS = 16\n",
    "\n",
    "dataset_dir = os.path.join(cwd, \"VQA_Dataset\")\n",
    "test_dir = os.path.join(dataset_dir, 'Images')\n",
    "\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_gen = test_data_gen.flow_from_directory(dataset_dir,\n",
    "                                             batch_size=BS,\n",
    "                                             color_mode=\"rgb\",\n",
    "                                             target_size=(IMG_H, IMG_W),\n",
    "                                             shuffle=False)\n",
    "test_gen.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBgux3Or4_pG",
    "outputId": "6db31b8b-8f4e-4ed0-f136-990e3998dc42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 100, 175, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 100, 175, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 100, 175, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 50, 87, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 50, 87, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 50, 87, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 25, 43, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 25, 43, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 25, 43, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 25, 43, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 12, 21, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 12, 21, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 12, 21, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 12, 21, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 6, 10, 512)        0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 6, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 6, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 6, 10, 512)        2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 3, 5, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#VGG model\n",
    "vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMG_H, IMG_W, 3)) \n",
    "vgg.trainable = False\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3RK4g2f5BtG",
    "outputId": "f2a3922f-1ab8-408d-fa04-a5846833309c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1834/1834 [==============================] - 249s 132ms/step\n"
     ]
    }
   ],
   "source": [
    "pre_model = tf.keras.Sequential()\n",
    "pre_model.add(vgg)\n",
    "pre_model.trainable = False\n",
    "img_predictions = pre_model.predict(test_gen, len(test_gen), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvoAMjGR5FkV"
   },
   "outputs": [],
   "source": [
    "feature_maps = {}\n",
    "image_names = []\n",
    "\n",
    "from os import listdir\n",
    "\n",
    "for name in listdir(test_dir):\n",
    "  image_names.append(name)\n",
    "\n",
    "image_names.sort()\n",
    "\n",
    "for i in range(len(image_names)):\n",
    "  img_name=image_names[i].replace('.png' , '')\n",
    "  feature_maps[img_name] = img_predictions[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfhM776xVZfA"
   },
   "source": [
    "## Creation on input Questions and answer vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxyj8LagWV_2"
   },
   "outputs": [],
   "source": [
    "# OPEN QUESTIONS\n",
    "file=os.path.join(dataset_dir, 'train_questions_annotations.json')\n",
    "f = open(file)\n",
    "train_q = json.load(f)\n",
    "\n",
    "#Load Spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUM4_d2A2Hok"
   },
   "outputs": [],
   "source": [
    "#Question list\n",
    "questions = list([text for id in train_q.keys() for text in train_q[id]['question'].splitlines()])\n",
    "\n",
    "#Answer list\n",
    "answers = list([text for id in train_q.keys() for text in train_q[id]['answer'].splitlines()])\n",
    "\n",
    "answer_vocab = list(set([text for id in train_q.keys() for text in train_q[id]['answer'].splitlines()]))\n",
    "answer_vocab_size = len(answer_vocab)\n",
    "\n",
    "ans_word_to_idx = { w: i for i, w in enumerate(answer_vocab) }\n",
    "ans_idx_to_word = { i: w for i, w in enumerate(answer_vocab) }\n",
    "\n",
    "#Img_id list\n",
    "img_ids = list([text for id in train_q.keys() for text in train_q[id]['image_id'].splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LL-_HZXi8vim"
   },
   "outputs": [],
   "source": [
    "#Check correctness\n",
    "print(list(train_q.items())[0])\n",
    "print(answers[0])\n",
    "print(img_ids[0])\n",
    "print(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dRZT9Tx-eFD"
   },
   "outputs": [],
   "source": [
    "### Find max len quesiton\n",
    "max_len = 0\n",
    "max_len_q_index = 0\n",
    "for i in range(len(questions)):\n",
    "  q_len = len(questions[i].strip().split(\" \"))\n",
    "  if(q_len > max_len):\n",
    "    max_len = q_len\n",
    "    max_len_q_index = i\n",
    "\n",
    "max_len = len(nlp(questions[max_len_q_index]))\n",
    "\n",
    "print(max_len)\n",
    "print(list(train_q.items())[max_len_q_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgT7uzP150u3"
   },
   "source": [
    "## Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7t9OZg4g526r"
   },
   "outputs": [],
   "source": [
    "#create inputs\n",
    "def createInputs(text):\n",
    "  '''\n",
    "  Returns an array of one-hot vectors representing the words\n",
    "  in the input text string.\n",
    "  - text is a string\n",
    "  - Each one-hot vector has shape (vocab_size, 1)\n",
    "  '''\n",
    "  inputs = []\n",
    "  for w in text.split(' '):\n",
    "    v = np.zeros((vocab_size, 1)) \n",
    "    v[word_to_idx[w]] = 1\n",
    "    inputs.append(v)\n",
    "  return inputs\n",
    "\n",
    "def get_questions_tensor_timeseries(questions, nlp, timesteps):\n",
    "    #assert not isinstance(questions, list) --- CHECK WHEN CREATING BATCHES\n",
    "    nb_samples = len(questions)\n",
    "    word_vec_dim = nlp(questions[0])[0].vector.shape[0]\n",
    "    questions_tensor = np.zeros((nb_samples, timesteps, word_vec_dim))\n",
    "    for i in range(len(questions)):\n",
    "        tokens = nlp(questions[i])\n",
    "        for j in range(len(tokens)):\n",
    "            if j<timesteps:\n",
    "                questions_tensor[i,j,:] = tokens[j].vector\n",
    "    return questions_tensor\n",
    "\n",
    "#Encode answers\n",
    "def encodeAns(w, ans_word_to_idx):\n",
    "  '''\n",
    "  Returns a one-hot vectors representing the answer word in the answer vocab.\n",
    "  Each one-hot vector has shape (answer_vocab_size, 1)\n",
    "  '''\n",
    "  ohe_word = np.zeros((answer_vocab_size, 1)) \n",
    "  ohe_word[ans_word_to_idx[w]] = 1\n",
    "  return ohe_word \n",
    "\n",
    "#Decode answers\n",
    "def decodeAns(ohe_w, ans_idx_to_word):\n",
    "  '''\n",
    "  Returns the word from the one-hot-encoding answer vector\n",
    "  '''\n",
    "  i = 0\n",
    "  while(ohe_w[i] == 0):\n",
    "    i = i+1\n",
    "  return ans_idx_to_word[i] \n",
    "\n",
    "def get_questions_tensor_timeseries(questions, nlp, timesteps):\n",
    "    #assert not isinstance(questions, list) #--- CHECK WHEN CREATING BATCHES\n",
    "    nb_samples = len(questions)\n",
    "    word_vec_dim = nlp(questions[0])[0].vector.shape[0]\n",
    "    questions_tensor = np.zeros((nb_samples, timesteps, word_vec_dim))\n",
    "    for i in range(len(questions)):\n",
    "        tokens = nlp(questions[i])\n",
    "        for j in range(len(tokens)):\n",
    "            if j<timesteps:\n",
    "                questions_tensor[i,j,:] = tokens[j].vector\n",
    "    return questions_tensor\n",
    "\n",
    "def get_question_tensor_timeseries(question, nlp, timesteps): \n",
    "    word_vec_dim = nlp(question)[0].vector.shape[0]\n",
    "    question_tensor = np.zeros((timesteps, word_vec_dim))\n",
    "    tokens = nlp(question)\n",
    "    for j in range(len(tokens)):\n",
    "      if j<timesteps:\n",
    "        question_tensor[j,:] = tokens[j].vector\n",
    "    return question_tensor \n",
    "\n",
    "def grouped(iterable, n, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "   #print(args)\n",
    "    #for a in(zip_longest(*args, fillvalue=fillvalue)):\n",
    "        #print(a)\n",
    "    return zip_longest(*args, fillvalue=fillvalue)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAGfc1mw6B8N"
   },
   "source": [
    "## Input Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSBhggSr8it8"
   },
   "outputs": [],
   "source": [
    "def create_tuples(questions, answers, img_ids, max_len, nlp, ans_word_to_idx, feature_maps):\n",
    "    \n",
    "    new_answers_train = list()\n",
    "    new_questions_train = list()\n",
    "    new_images_train = list()\n",
    "    for ans, ques, img_id in zip(answers, questions, img_ids):\n",
    "      \n",
    "      ohe_ans = encodeAns(ans, ans_word_to_idx) #one hot encoding of answer\n",
    "      encoded_question = get_question_tensor_timeseries(ques, nlp, max_len)  #question embedding\n",
    "      img_feature_maps = feature_maps[img_id] #feature maps of that img\n",
    "\n",
    "      new_answers_train.append(ohe_ans)\n",
    "      new_questions_train.append(encoded_question)\n",
    "      new_images_train.append(img_feature_maps.flatten())\n",
    "\n",
    "    return (new_questions_train, new_answers_train, new_images_train)\n",
    "\n",
    "def create_test_tuples(questions, img_ids, max_len, nlp, feature_maps):\n",
    "    \n",
    "    new_questions_train = list()\n",
    "    new_images_train = list()\n",
    "\n",
    "    for ques, img_id in zip(questions, img_ids):\n",
    "      \n",
    "      encoded_question = get_question_tensor_timeseries(ques, nlp, max_len)  #question embedding\n",
    "      img_feature_maps = feature_maps[img_id] #feature maps of that img\n",
    "\n",
    "      new_questions_train.append(encoded_question)\n",
    "      new_images_train.append(img_feature_maps.flatten())\n",
    "\n",
    "    return (new_questions_train, new_images_train)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4PUkCtR2KCg"
   },
   "outputs": [],
   "source": [
    "#create lists of encoded items\n",
    "en_questions, en_answers, en_img_ids = create_tuples(questions, answers, img_ids, max_len, nlp, ans_word_to_idx, feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ax0wUm4ZJNQ1",
    "outputId": "8dbbf166-85f0-4ccf-d571-aad4b3af7c7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58832 58832 58832\n"
     ]
    }
   ],
   "source": [
    "#check lenght\n",
    "print (len(en_questions), len(en_answers),len(en_img_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxOv_B_N6Jxc"
   },
   "source": [
    "## Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-B5ffI6nyJV",
    "outputId": "0a2a5295-bc71-40b2-9fb0-33ec69de40c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_input (InputLayer)   [(None, 7680)]            0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7680)              0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#With load feature map from file should just be an empty model\n",
    "image_model = Sequential()\n",
    "image_model.add(Reshape(input_shape = (7680,), target_shape=(7680,))) ### set correct shape 7680 (3x5x512)\n",
    "model1 = Model(inputs = image_model.input, outputs = image_model.output)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4E_1MdWV0-e"
   },
   "source": [
    "## LSTM Model\n",
    "LSTM model that takes the question as input and creates the output to send to our final dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCzOv3cQpkBP",
    "outputId": "1a2966d8-f1fa-480b-a1d8-e0445a13abcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_input (InputLayer)      [(None, None, 300)]       0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 512)         1665024   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 512)         2099200   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 512)               2099200   \n",
      "=================================================================\n",
      "Total params: 5,863,424\n",
      "Trainable params: 5,863,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Question Model\n",
    "# LSTM RNN\n",
    "word2vec_dim = 300 \n",
    "num_layers_lstm = 3\n",
    "num_hidden_nodes_lstm = 512\n",
    "output_dim = num_hidden_nodes_lstm\n",
    "\n",
    "language_model = Sequential()\n",
    "language_model.add(LSTM(units=output_dim, \n",
    "                        return_sequences=True, input_shape=(None, word2vec_dim)))\n",
    "\n",
    "for i in range(num_layers_lstm-2):\n",
    "    language_model.add(LSTM(units=output_dim, return_sequences=True))\n",
    "language_model.add(LSTM(units=output_dim, return_sequences=False))\n",
    "\n",
    "model2 = Model(language_model.input, language_model.output)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqvRWJaSWA3z"
   },
   "source": [
    "## Final model \n",
    "Model that merges the questions and the images and has dense layers after it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5aoze4zp4a2"
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "combined = concatenate([image_model.output, language_model.output])\n",
    "#model = GlobalAveragePooling2D() (combined)\n",
    "#model = Flatten (model)\n",
    "model = Dense(256, activation = 'relu',kernel_regularizer=tf.keras.regularizers.l2(0.001))(combined)\n",
    "model = Dropout(0.5)(model)\n",
    "\n",
    "model = Dense(58)(model)\n",
    "model = Activation(\"softmax\")(model)\n",
    "\n",
    "\n",
    "model = Model(inputs=[image_model.input, language_model.input], outputs=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R87oDkx4Ng4T",
    "outputId": "9bcbf0e7-958c-46c1-f21d-dd380ee25648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         [(None, None, 300)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, None, 512)    1665024     lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "reshape_input (InputLayer)      [(None, 7680)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, None, 512)    2099200     lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 7680)         0           reshape_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 512)          2099200     lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8192)         0           reshape[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 58)           14906       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 58)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,975,738\n",
      "Trainable params: 7,975,738\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# Validation metrics\n",
    "\n",
    "metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics = metric)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1YJ8xjfzNCo"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFj4ZVqRO_5G"
   },
   "outputs": [],
   "source": [
    "train_questions = en_questions[:45000]\n",
    "train_answers = en_answers[:45000]\n",
    "train_image_id = en_img_ids[:45000]\n",
    "\n",
    "valid_questions = en_questions[45000:]\n",
    "valid_answers = en_answers[45000:]\n",
    "valid_image_id = en_img_ids[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6do_uJRPIAB",
    "outputId": "45781ab8-5ea1-4277-d07e-de3fe5e3e922"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number:  1\n",
      "45012/45000 [==============================] - 50s 1ms/step - train loss: 0.4427 - train accuracy: 0.8737 - val loss: 1.7620 - val accuracy: 0.6100\n",
      "Epoch Number:  2\n",
      "45012/45000 [==============================] - 50s 1ms/step - train loss: 0.4298 - train accuracy: 0.8770 - val loss: 1.7813 - val accuracy: 0.6076\n",
      "Epoch Number:  3\n",
      "45012/45000 [==============================] - 50s 1ms/step - train loss: 0.4220 - train accuracy: 0.8791 - val loss: 1.8080 - val accuracy: 0.6089\n",
      "Epoch Number:  4\n",
      "45012/45000 [==============================] - 50s 1ms/step - train loss: 0.4194 - train accuracy: 0.8811 - val loss: 1.8289 - val accuracy: 0.6062\n",
      "Epoch Number:  5\n",
      "45012/45000 [==============================] - 50s 1ms/step - train loss: 0.4221 - train accuracy: 0.8792 - val loss: 1.8131 - val accuracy: 0.6091\n"
     ]
    }
   ],
   "source": [
    "batch_size = 33\n",
    "vbatch_size= 10\n",
    "num_epochs = 5\n",
    "\n",
    "for k in range(num_epochs):\n",
    "    print(\"Epoch Number: \",k+1)\n",
    "    progbar = generic_utils.Progbar(len(train_questions))\n",
    "    for question_batch, ans_batch, im_batch, vquestion_batch, vans_batch, vim_batch, in zip(grouped(train_questions, batch_size, fillvalue=train_questions[-1]), \n",
    "                                                                                            grouped(train_answers, batch_size, fillvalue=train_answers[-1]),\n",
    "                                                                                            grouped(train_image_id, batch_size, fillvalue=train_image_id[-1]),\n",
    "                                                                                            grouped(valid_questions, vbatch_size, fillvalue=valid_questions[-1]), \n",
    "                                                                                            grouped(valid_answers, vbatch_size, fillvalue=valid_answers[-1]),\n",
    "                                                                                            grouped(valid_image_id, vbatch_size, fillvalue=valid_image_id[-1])):\n",
    "                                                X_ques_batch = question_batch\n",
    "                                                X_img_batch = im_batch\n",
    "                                                Y_batch = ans_batch\n",
    "                                                X_vques_batch = vquestion_batch\n",
    "                                                X_vimg_batch = vim_batch\n",
    "                                                Y_vbatch = vans_batch\n",
    "                                                np_X_ques_batch = np.array(X_ques_batch)\n",
    "                                                np_X_img_batch = np.array(X_img_batch)\n",
    "                                                np_Y_batch = np.array(Y_batch)\n",
    "                                                np_X_vques_batch = np.array(X_vques_batch)\n",
    "                                                np_X_vimg_batch = np.array(X_vimg_batch)\n",
    "                                                np_Y_vbatch = np.array(Y_vbatch)\n",
    "                                                loss, acc = model.train_on_batch(({'lstm_input' : np_X_ques_batch, 'reshape_input' : np_X_img_batch}), np_Y_batch, class_weight=None)\n",
    "                                                vloss, vacc = model.test_on_batch(({'lstm_input' : np_X_vques_batch, 'reshape_input' : np_X_vimg_batch}), np_Y_vbatch)\n",
    "                                                #loss = model.train_on_batch(({'lstm_1_input' : np_X_ques_batch, 'reshape_1_input' : np_X_img_batch}), np_Y_batch, class_weight=None)\n",
    "                                                progbar.add(batch_size, values=[('train loss', loss),('train accuracy', acc),('val loss', vloss),('val accuracy', vacc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tEeT2tP5jtX"
   },
   "outputs": [],
   "source": [
    "widgets = ['Evaluating ', Percentage(), ' ', Bar(marker='#',left='[',right=']'), ' ', ETA()]\n",
    "pbar = ProgressBar(widgets=widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3g2iwTd5BBK"
   },
   "outputs": [],
   "source": [
    "#To manually check validation scores\n",
    "valid_pred = []\n",
    "batch_size = 1 \n",
    "\n",
    "for qu_batch,an_batch,im_batch in pbar(zip(grouped(valid_questions, batch_size, \n",
    "                                                   fillvalue=valid_questions[0]), \n",
    "                                           grouped(valid_answers, batch_size, \n",
    "                                                   fillvalue=valid_answers[0]), \n",
    "                                           grouped(valid_image_id, batch_size, \n",
    "                                                   fillvalue=valid_image_id[0]))):\n",
    "    X_ques_batch = qu_batch\n",
    "    X_img_batch = im_batch\n",
    "    np_X_ques_batch = np.array(X_ques_batch)\n",
    "    np_X_img_batch = np.array(X_img_batch)\n",
    "    valid_predict = model.predict(({'lstm_input' : np_X_ques_batch, 'reshape_input' : np_X_img_batch}))\n",
    "    valid_predict = np.argmax(valid_predict,axis=1)\n",
    "    valid_pred.extend(valid_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cc-Yz29I8OZd"
   },
   "outputs": [],
   "source": [
    "#Decode validation answers to words\n",
    "for i in range(len(valid_pred)):\n",
    "  vector = np.zeros(len(answer_vocab))\n",
    "  vector[valid_pred[i]] = 1\n",
    "  valid_pred[i] = decodeAns(vector, ans_idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI8p-2IICEb_"
   },
   "outputs": [],
   "source": [
    "#Manual accuracy over validation set\n",
    "correct = 0\n",
    "for j in range(len(valid_pred)):\n",
    "  if(valid_pred[j] == decodeAns(valid_answers[j], ans_idx_to_word)):\n",
    "    correct = correct + 1\n",
    "\n",
    "print(correct)\n",
    "\n",
    "accuracy = correct/len(valid_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lc9ZwkE4zSGE"
   },
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCwI8VHnzESB"
   },
   "outputs": [],
   "source": [
    "path = os.path.join(dataset_dir, 'test_questions.json')\n",
    "d = open(path)\n",
    "test_q = json.load(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hmxdx0LozlUZ"
   },
   "outputs": [],
   "source": [
    "#Test questions\n",
    "test_questions = list([text for id in test_q.keys() for text in test_q[id]['question'].splitlines()])\n",
    "\n",
    "#Test Img_id list \n",
    "test_img_ids = list([text for id in test_q.keys() for text in test_q[id]['image_id'].splitlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WwLwUaMa0kyf",
    "outputId": "971f634b-1b57-4196-e7bc-f40b8b24f93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "24\n",
      "('21471', {'question': 'Is the lady standing on the rug and the woman in the portrait wearing the same colored shirt?', 'image_id': '2147'})\n"
     ]
    }
   ],
   "source": [
    "#Compute test max lenght -- Just to check if less or equal than training\n",
    "test_max_len = 0\n",
    "test_max_len_q_index = 0\n",
    "for i in range(len(test_questions)):\n",
    "  test_q_len = len(test_questions[i].strip().split(\" \"))\n",
    "  if(test_q_len > test_max_len):\n",
    "    test_max_len = test_q_len\n",
    "    test_max_len_q_index = i\n",
    "\n",
    "test_max_len = len(nlp(test_questions[test_max_len_q_index]))\n",
    "\n",
    "print(test_max_len)\n",
    "print(max_len)\n",
    "print(list(test_q.items())[test_max_len_q_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DM5vMK641a2h"
   },
   "outputs": [],
   "source": [
    "#encode test data\n",
    "en_test_questions, en_test_img_ids = create_test_tuples(test_questions, test_img_ids, max_len, nlp, feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9B620O6CzfTf",
    "outputId": "5b1e5c70-e59e-4bcd-f23f-d1c4814d9ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6372\n",
      "6372\n"
     ]
    }
   ],
   "source": [
    "print(len(test_questions))\n",
    "print(len(en_test_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxmwdDZ72TOt",
    "outputId": "345d4fbb-8322-4830-efc9-5d11732f0949"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating N/A% [#                                             ] Time:  1:36:30"
     ]
    }
   ],
   "source": [
    "#create predictions\n",
    "test_batch_size = 1\n",
    "y_pred = []\n",
    "\n",
    "\n",
    "for qu_batch,im_batch in pbar(zip(grouped(en_test_questions, test_batch_size, \n",
    "                                                   fillvalue=en_test_questions[0]),                                            \n",
    "                                           grouped(en_test_img_ids, test_batch_size, \n",
    "                                                   fillvalue=en_test_img_ids[0]))):\n",
    "\n",
    "    X_ques_batch = qu_batch\n",
    "    X_img_batch = im_batch\n",
    "    np_X_ques_batch = np.array(X_ques_batch)\n",
    "    np_X_img_batch = np.array(X_img_batch)\n",
    "    y_predict = model.predict(({'lstm_input' : np_X_ques_batch, 'reshape_input' : np_X_img_batch}))\n",
    "    y_predict = np.argmax(y_predict,axis=1)\n",
    "    y_pred.extend(y_predict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTDqZBEjqMJe"
   },
   "source": [
    "## Creation of submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YchCUyNTZguc"
   },
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "        '0': 0,\n",
    "        '1': 1,\n",
    "        '2': 2,\n",
    "        '3': 3,\n",
    "        '4': 4,\n",
    "        '5': 5,\n",
    "        'apple': 6,\n",
    "        'baseball': 7,\n",
    "        'bench': 8,\n",
    "        'bike': 9,\n",
    "        'bird': 10,\n",
    "        'black': 11,\n",
    "        'blanket': 12,\n",
    "        'blue': 13,\n",
    "        'bone': 14,\n",
    "        'book': 15,\n",
    "        'boy': 16,\n",
    "        'brown': 17,\n",
    "        'cat': 18,\n",
    "        'chair': 19,\n",
    "        'couch': 20,\n",
    "        'dog': 21,\n",
    "        'floor': 22,\n",
    "        'food': 23,\n",
    "        'football': 24,\n",
    "        'girl': 25,\n",
    "        'grass': 26,\n",
    "        'gray': 27,\n",
    "        'green': 28,\n",
    "        'left': 29,\n",
    "        'log': 30,\n",
    "        'man': 31,\n",
    "        'monkey bars': 32,\n",
    "        'no': 33,\n",
    "        'nothing': 34,\n",
    "        'orange': 35,\n",
    "        'pie': 36,\n",
    "        'plant': 37,\n",
    "        'playing': 38,\n",
    "        'red': 39,\n",
    "        'right': 40,\n",
    "        'rug': 41,\n",
    "        'sandbox': 42,\n",
    "        'sitting': 43,\n",
    "        'sleeping': 44,\n",
    "        'soccer': 45,\n",
    "        'squirrel': 46,\n",
    "        'standing': 47,\n",
    "        'stool': 48,\n",
    "        'sunny': 49,\n",
    "        'table': 50,\n",
    "        'tree': 51,\n",
    "        'watermelon': 52,\n",
    "        'white': 53,\n",
    "        'wine': 54,\n",
    "        'woman': 55,\n",
    "        'yellow': 56,\n",
    "        'yes': 57\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUhT-TpBotfM"
   },
   "outputs": [],
   "source": [
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(key + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlgZSaTkOXvm"
   },
   "outputs": [],
   "source": [
    "#Decode answers to words\n",
    "de_y_pred = y_pred\n",
    "for i in range(len(de_y_pred)):\n",
    "  vector = np.zeros(len(answer_vocab))\n",
    "  vector[de_y_pred[i]] = 1\n",
    "  de_y_pred[i] = decodeAns(vector, ans_idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZfYZCqPP1YX"
   },
   "outputs": [],
   "source": [
    "#Re encode with correct dictionary\n",
    "final_y_pred = de_y_pred\n",
    "for i in range(len(final_y_pred)):\n",
    "  final_y_pred[i] = labels_dict.get(final_y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxWaAKtCWHgU"
   },
   "outputs": [],
   "source": [
    "#create submission\n",
    "results = {}\n",
    "test_ids = list(test_q.keys())\n",
    "for i in range(len(test_ids)):\n",
    "  results[test_ids[i]] = final_y_pred[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41_v103bWUpl"
   },
   "outputs": [],
   "source": [
    "create_csv(results, '/content/drive/My Drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzmoK3ZMQBKB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UfhM776xVZfA",
    "lgT7uzP150u3",
    "kxOv_B_N6Jxc",
    "Z4E_1MdWV0-e",
    "PqvRWJaSWA3z",
    "t1YJ8xjfzNCo"
   ],
   "name": "updatedHW3_614.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
